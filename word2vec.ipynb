{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-01 11:25:56,004 : INFO : running C:\\Users\\HP\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel_launcher.py --ip=127.0.0.1 --stdin=9003 --control=9001 --hb=9000 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"d2bd2183-03ca-43fc-a60c-1ebc9a7563f0\" --shell=9002 --transport=\"tcp\" --iopub=9004 --f=c:\\Users\\HP\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-18756pQseV4YfAsoP.json\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Invalid data stream",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32md:\\KULIYEAH\\kuli-ah\\semester7\\STKI\\tugas\\minggu5\\word2vec.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/KULIYEAH/kuli-ah/semester7/STKI/tugas/minggu5/word2vec.ipynb#W0sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(namaFileOutput, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/KULIYEAH/kuli-ah/semester7/STKI/tugas/minggu5/word2vec.ipynb#W0sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m wiki \u001b[39m=\u001b[39m WikiCorpus(namaFileInput, lemmatize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dictionary\u001b[39m=\u001b[39m{}, lower\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/KULIYEAH/kuli-ah/semester7/STKI/tugas/minggu5/word2vec.ipynb#W0sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m wiki\u001b[39m.\u001b[39mget_texts():\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/KULIYEAH/kuli-ah/semester7/STKI/tugas/minggu5/word2vec.ipynb#W0sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     output\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(text) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/KULIYEAH/kuli-ah/semester7/STKI/tugas/minggu5/word2vec.ipynb#W0sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     i \u001b[39m=\u001b[39m i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\gensim\\corpora\\wikicorpus.py:692\u001b[0m, in \u001b[0;36mWikiCorpus.get_texts\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    687\u001b[0m pool \u001b[39m=\u001b[39m multiprocessing\u001b[39m.\u001b[39mPool(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocesses, init_to_ignore_interrupt)\n\u001b[0;32m    689\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    690\u001b[0m     \u001b[39m# process the corpus in smaller chunks of docs, because multiprocessing.Pool\u001b[39;00m\n\u001b[0;32m    691\u001b[0m     \u001b[39m# is dumb and would load the entire input into RAM at once...\u001b[39;00m\n\u001b[1;32m--> 692\u001b[0m     \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m utils\u001b[39m.\u001b[39mchunkize(texts, chunksize\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocesses, maxsize\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m    693\u001b[0m         \u001b[39mfor\u001b[39;00m tokens, title, pageid \u001b[39min\u001b[39;00m pool\u001b[39m.\u001b[39mimap(_process_article, group):\n\u001b[0;32m    694\u001b[0m             articles_all \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\gensim\\utils.py:1334\u001b[0m, in \u001b[0;36mchunkize\u001b[1;34m(corpus, chunksize, maxsize, as_numpy)\u001b[0m\n\u001b[0;32m   1332\u001b[0m     entity \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWindows\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnt\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mOSX with python3.8+\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1333\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mdetected \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m; aliasing chunkize to chunkize_serial\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m entity)\n\u001b[1;32m-> 1334\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m chunkize_serial(corpus, chunksize, as_numpy\u001b[39m=\u001b[39mas_numpy):\n\u001b[0;32m   1335\u001b[0m     \u001b[39myield\u001b[39;00m chunk\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\gensim\\utils.py:1244\u001b[0m, in \u001b[0;36mchunkize_serial\u001b[1;34m(iterable, chunksize, as_numpy, dtype)\u001b[0m\n\u001b[0;32m   1242\u001b[0m     wrapped_chunk \u001b[39m=\u001b[39m [[np\u001b[39m.\u001b[39marray(doc, dtype\u001b[39m=\u001b[39mdtype) \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mislice(it, \u001b[39mint\u001b[39m(chunksize))]]\n\u001b[0;32m   1243\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1244\u001b[0m     wrapped_chunk \u001b[39m=\u001b[39m [\u001b[39mlist\u001b[39;49m(itertools\u001b[39m.\u001b[39;49mislice(it, \u001b[39mint\u001b[39;49m(chunksize)))]\n\u001b[0;32m   1245\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m wrapped_chunk[\u001b[39m0\u001b[39m]:\n\u001b[0;32m   1246\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\gensim\\corpora\\wikicorpus.py:682\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    679\u001b[0m positions, positions_all \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[0;32m    681\u001b[0m tokenization_params \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer_func, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_min_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_max_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlower)\n\u001b[1;32m--> 682\u001b[0m texts \u001b[39m=\u001b[39m (\n\u001b[0;32m    683\u001b[0m     (text, title, pageid, tokenization_params)\n\u001b[0;32m    684\u001b[0m     \u001b[39mfor\u001b[39;00m title, text, pageid\n\u001b[0;32m    685\u001b[0m     \u001b[39min\u001b[39;00m extract_pages(bz2\u001b[39m.\u001b[39mBZ2File(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfname), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilter_namespaces, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilter_articles)\n\u001b[0;32m    686\u001b[0m )\n\u001b[0;32m    687\u001b[0m pool \u001b[39m=\u001b[39m multiprocessing\u001b[39m.\u001b[39mPool(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocesses, init_to_ignore_interrupt)\n\u001b[0;32m    689\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    690\u001b[0m     \u001b[39m# process the corpus in smaller chunks of docs, because multiprocessing.Pool\u001b[39;00m\n\u001b[0;32m    691\u001b[0m     \u001b[39m# is dumb and would load the entire input into RAM at once...\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\gensim\\corpora\\wikicorpus.py:412\u001b[0m, in \u001b[0;36mextract_pages\u001b[1;34m(f, filter_namespaces, filter_articles)\u001b[0m\n\u001b[0;32m    406\u001b[0m elems \u001b[39m=\u001b[39m (elem \u001b[39mfor\u001b[39;00m _, elem \u001b[39min\u001b[39;00m iterparse(f, events\u001b[39m=\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m,)))\n\u001b[0;32m    408\u001b[0m \u001b[39m# We can't rely on the namespace for database dumps, since it's changed\u001b[39;00m\n\u001b[0;32m    409\u001b[0m \u001b[39m# it every time a small modification to the format is made. So, determine\u001b[39;00m\n\u001b[0;32m    410\u001b[0m \u001b[39m# those from the first element we find, which will be part of the metadata,\u001b[39;00m\n\u001b[0;32m    411\u001b[0m \u001b[39m# and construct element paths.\u001b[39;00m\n\u001b[1;32m--> 412\u001b[0m elem \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(elems)\n\u001b[0;32m    413\u001b[0m namespace \u001b[39m=\u001b[39m get_namespace(elem\u001b[39m.\u001b[39mtag)\n\u001b[0;32m    414\u001b[0m ns_mapping \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mns\u001b[39m\u001b[39m\"\u001b[39m: namespace}\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\gensim\\corpora\\wikicorpus.py:406\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_pages\u001b[39m(f, filter_namespaces\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, filter_articles\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    391\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Extract pages from a MediaWiki database dump.\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \n\u001b[0;32m    393\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    404\u001b[0m \n\u001b[0;32m    405\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 406\u001b[0m     elems \u001b[39m=\u001b[39m (elem \u001b[39mfor\u001b[39;00m _, elem \u001b[39min\u001b[39;00m iterparse(f, events\u001b[39m=\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m,)))\n\u001b[0;32m    408\u001b[0m     \u001b[39m# We can't rely on the namespace for database dumps, since it's changed\u001b[39;00m\n\u001b[0;32m    409\u001b[0m     \u001b[39m# it every time a small modification to the format is made. So, determine\u001b[39;00m\n\u001b[0;32m    410\u001b[0m     \u001b[39m# those from the first element we find, which will be part of the metadata,\u001b[39;00m\n\u001b[0;32m    411\u001b[0m     \u001b[39m# and construct element paths.\u001b[39;00m\n\u001b[0;32m    412\u001b[0m     elem \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(elems)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\xml\\etree\\ElementTree.py:1256\u001b[0m, in \u001b[0;36miterparse.<locals>.iterator\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1254\u001b[0m \u001b[39myield from\u001b[39;00m pullparser\u001b[39m.\u001b[39mread_events()\n\u001b[0;32m   1255\u001b[0m \u001b[39m# load event buffer\u001b[39;00m\n\u001b[1;32m-> 1256\u001b[0m data \u001b[39m=\u001b[39m source\u001b[39m.\u001b[39;49mread(\u001b[39m16\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39m1024\u001b[39;49m)\n\u001b[0;32m   1257\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[0;32m   1258\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\bz2.py:164\u001b[0m, in \u001b[0;36mBZ2File.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Read up to size uncompressed bytes from the file.\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \n\u001b[0;32m    160\u001b[0m \u001b[39mIf size is negative or omitted, read until EOF is reached.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[39mReturns b'' if the file is already at EOF.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_can_read()\n\u001b[1;32m--> 164\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_buffer\u001b[39m.\u001b[39;49mread(size)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreadinto\u001b[39m(\u001b[39mself\u001b[39m, b):\n\u001b[0;32m     67\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b) \u001b[39mas\u001b[39;00m view, view\u001b[39m.\u001b[39mcast(\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m byte_view:\n\u001b[1;32m---> 68\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m(byte_view))\n\u001b[0;32m     69\u001b[0m         byte_view[:\u001b[39mlen\u001b[39m(data)] \u001b[39m=\u001b[39m data\n\u001b[0;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(data)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\_compression.py:103\u001b[0m, in \u001b[0;36mDecompressReader.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    102\u001b[0m         rawblock \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 103\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decompressor\u001b[39m.\u001b[39;49mdecompress(rawblock, size)\n\u001b[0;32m    104\u001b[0m \u001b[39mif\u001b[39;00m data:\n\u001b[0;32m    105\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: Invalid data stream"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "\n",
    "import logging\n",
    "import os.path\n",
    "import sys\n",
    "\n",
    "from gensim.corpora import WikiCorpus\n",
    "\n",
    "program = os.path.basename(sys.argv[0])\n",
    "logger = logging.getLogger(program)\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "logger.info(\"running %s\" % ' '.join(sys.argv))\n",
    "\n",
    "namaFileInput = \"idwiki-latest-pages-articles.xml.bz2\"\n",
    "namaFileOutput = \"wiki.id.text\"\n",
    "\n",
    "space = \" \"\n",
    "i = 0\n",
    "\n",
    "output = open(namaFileOutput, 'w', encoding='utf-8')\n",
    "\n",
    "wiki = WikiCorpus(namaFileInput, lemmatize=None, dictionary={}, lower=False)\n",
    "for text in wiki.get_texts():\n",
    "    output.write(' '.join(text) + '\\n')\n",
    "    i = i + 1\n",
    "    if i % 10000 == 0:\n",
    "        logger.info(\"Saved \" + str(i) + \" articles\")\n",
    "\n",
    "output.close()\n",
    "logger.info(\"Finished Saved \" + str(i) + \" articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import logging\n",
    "import os.path\n",
    "import sys\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "program = os.path.basename(sys.argv[0])\n",
    "logger = logging.getLogger(program)\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "logger.info(\"running %s\" % ' '.join(sys.argv))\n",
    "\n",
    "namaFileInput = \"Wiki.id.text\"\n",
    "namaFileOutput = \"model_wiki_id.txt\"\n",
    "\n",
    "model = Word2Vec(LineSentence(namaFileInput), vector_size=300, window=10, min_count=5, sg=0, workers=multiprocessing.cpu_count())\n",
    "\n",
    "model.init_sims(replace=True)\n",
    "model.wv.save_word2vec_format(namaFileOutput, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\KULIYEAH\\kuli-ah\\semester7\\STKI\\tugas\\minggu5\\word2vec.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/KULIYEAH/kuli-ah/semester7/STKI/tugas/minggu5/word2vec.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgensim\u001b[39;00m \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/KULIYEAH/kuli-ah/semester7/STKI/tugas/minggu5/word2vec.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m namaFileModel \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmodel_wiki_id.txt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/KULIYEAH/kuli-ah/semester7/STKI/tugas/minggu5/word2vec.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m sim \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39mmost_similar(\u001b[39m\"\u001b[39m\u001b[39mJakarta\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/KULIYEAH/kuli-ah/semester7/STKI/tugas/minggu5/word2vec.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m10 kata terdekat dari Jakarta:\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(sim))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/KULIYEAH/kuli-ah/semester7/STKI/tugas/minggu5/word2vec.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m sim \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39mmost_similar(\u001b[39m\"\u001b[39m\u001b[39mBandung\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import gensim \n",
    "\n",
    "namaFileModel = \"model_wiki_id.txt\"\n",
    "\n",
    "sim = model.wv.most_similar(\"Jakarta\")\n",
    "print(\"10 kata terdekat dari Jakarta:{}\".format(sim))\n",
    "sim = model.wv.most_similar(\"Bandung\")\n",
    "print(\"10 kata terdekat dari Bandung:{}\".format(sim)) \n",
    "\n",
    "sim = model.wv.similarity(\"Yogyakarta\", \"Surakarta\")\n",
    "print(\"Kedekatan Yogyakarta-Surakarta: {}\".format(sim))\n",
    "sim = model.wv.similarity(\"Yogyakarta\", \"Semarang\")\n",
    "print(\"Kedekatan Yogyakarta-Semarang: {}\".format(sim))\n",
    " \n",
    "sim = model.wv.most_similar_cosmul(positive=['minuman', 'rendang'], negative=['makanan'])\n",
    "print(\"makanan-rendang, minuman-?: {}\".format(sim))\n",
    "sim = model.wv.most_similar_cosmul(positive=['mobil', 'honda'], negative=['motor'])\n",
    "print(\"motor-honda, mobil-?: {}\".format(sim))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
